{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PQqlhbkvhbJ3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from transformers import BertTokenizer,AutoModel,AdamW,AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "import random\n",
    "from collections import Counter\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "tzQrEEythbKJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    input_path = './'\n",
    "    model_path = 'macbert-large_300_' #  nghuyong/ernie-2.0-large-en studio-ousia/luke-large\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 300\n",
    "    epochs = 6  # 5\n",
    "    encoder_lr = 25e-6\n",
    "    decoder_lr = 25e-6\n",
    "    min_lr = 0.5e-6\n",
    "    eps = 1e-6\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0.01\n",
    "    num_fold = 5\n",
    "    batch_size = 32\n",
    "    seed = 1006\n",
    "    OUTPUT_DIR = './'\n",
    "    num_workers = 8\n",
    "    device='cuda'\n",
    "    print_freq = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collate function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ef949ac8ce94b73a7c7388b470666d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc3ad25c460484382f4b43dec0cdde7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/660 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720ba728a24d40a6b8ed7c80f3f1281d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/107k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d520b70077e493fb581408443f0eba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/263k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b2cb7de15674530a7ab8842a3aa0a52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a36b538f7ff44918e709afa665eb333",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Collate:\n",
    "    def __init__(self, tokenizer, isTrain=True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.isTrain = isTrain\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        output = dict()\n",
    "        output[\"input_ids\"] = [sample[\"input_ids\"] for sample in batch]\n",
    "        output[\"attention_mask\"] = [sample[\"attention_mask\"] for sample in batch]\n",
    "        if self.isTrain:\n",
    "            output[\"target\"] = [sample[\"target\"] for sample in batch]\n",
    "\n",
    "        # calculate max token length of this batch\n",
    "        batch_max = max([len(ids) for ids in output[\"input_ids\"]])\n",
    "\n",
    "        # add padding\n",
    "        if self.tokenizer.padding_side == \"right\":\n",
    "            output[\"input_ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in\n",
    "                                   output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"attention_mask\"]]\n",
    "        else:\n",
    "            output[\"input_ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in\n",
    "                                   output[\"input_ids\"]]\n",
    "            output[\"attention_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"attention_mask\"]]\n",
    "\n",
    "        # convert to tensors\n",
    "        output[\"input_ids\"] = torch.tensor(output[\"input_ids\"], dtype=torch.long)\n",
    "        output[\"attention_mask\"] = torch.tensor(output[\"attention_mask\"], dtype=torch.long)\n",
    "        if self.isTrain:\n",
    "            output[\"label\"] = torch.tensor(output[\"target\"], dtype=torch.long)\n",
    "\n",
    "        return output\n",
    "        \n",
    "#         return torch.as_tensor(inputs['input_ids'], dtype=torch.long), \\\n",
    "#                torch.as_tensor(inputs['attention_mask'], dtype=torch.long), \\\n",
    "#                torch.as_tensor(label, dtype=torch.long)\n",
    "    \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-macbert-large')\n",
    "collate_fn = Collate(tokenizer, isTrain=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def kl(inputs, targets, reduction=\"sum\"):\n",
    "# \"\"\"\n",
    "# 计算kl散度\n",
    "# inputs：tensor，logits\n",
    "# targets：tensor，logits\n",
    "# \"\"\"\n",
    "    loss = F.kl_div(F.log_softmax(inputs, dim=-1),\n",
    "                    F.softmax(targets, dim=-1),\n",
    "                    reduction=reduction)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def adv_project(grad, norm_type='inf', eps=1e-6):\n",
    "# \"\"\"\n",
    "# L0,L1,L2正则，对于扰动计算\n",
    "# \"\"\"\n",
    "    if norm_type == 'l2':\n",
    "        direction = grad / (torch.norm(grad, dim=-1, keepdim=True) + eps)\n",
    "    elif norm_type == 'l1':\n",
    "        direction = grad.sign()\n",
    "    else:\n",
    "        direction = grad / (grad.abs().max(-1, keepdim=True)[0] + eps)\n",
    "    return direction\n",
    "\n",
    "\n",
    "def virtual_adversarial_training(model, hidden_status, attention_mask, logits):\n",
    "# \"\"\"\n",
    "# 虚拟对抗式训练\n",
    "# model： nn.Module, 模型\n",
    "# hidden_status：tensor，input的embedded表示\n",
    "# token_type_ids：tensor，bert中的token_type_ids，A B 句子\n",
    "# attention_mask：tensor，bert中的attention_mask，对paddding mask\n",
    "# logits：tensor，input的输出\n",
    "# \"\"\"\n",
    "    embed = hidden_status\n",
    "    # 初始扰动 r\n",
    "    noise = embed.data.new(embed.size()).normal_(0, 1) * 1e-5\n",
    "    noise.requires_grad_()\n",
    "    # x + r\n",
    "    new_embed = embed.data.detach() + noise\n",
    "    adv_output = model(inputs_embeds=new_embed,\n",
    "                       input_ids = None,\n",
    "#                        token_type_ids=token_type_ids,\n",
    "                       attention_mask=attention_mask)\n",
    "    adv_logits = adv_output[0]\n",
    "    adv_loss = kl(adv_logits, logits.detach(), reduction=\"batchmean\")\n",
    "    delta_grad, = torch.autograd.grad(adv_loss, noise, only_inputs=True)\n",
    "    norm = delta_grad.norm()\n",
    "\n",
    "# 梯度消失，退出\n",
    "    if torch.isnan(norm) or torch.isinf(norm):\n",
    "        return None\n",
    "\n",
    "    # line 6 inner sum\n",
    "    noise = noise + delta_grad * 1e-3\n",
    "    # line 6 projection\n",
    "    noise = adv_project(noise, norm_type='l2', eps=1e-6)\n",
    "    new_embed = embed.data.detach() + noise\n",
    "    new_embed = new_embed.detach()\n",
    "    # 在进行一次训练\n",
    "    adv_output = model(inputs_embeds=new_embed,\n",
    "                       input_ids = None,\n",
    "#                        token_type_ids=token_type_ids,\n",
    "                       attention_mask=attention_mask)\n",
    "    adv_logits = adv_output[0]\n",
    "    adv_loss_f = kl(adv_logits, logits.detach())\n",
    "    adv_loss_b = kl(logits, adv_logits.detach())\n",
    "    # 在预训练时设置为10，下游任务设置为1\n",
    "    adv_loss = (adv_loss_f + adv_loss_b) * 1\n",
    "\n",
    "    return adv_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mykwcZyypr1O",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CFG.seed)\n",
    "\n",
    "class FGM():\n",
    "    \"\"\"\n",
    "    定义对抗训练方法FGM,对模型embedding参数进行扰动\n",
    "    \"\"\"\n",
    "    def __init__(self, model, epsilon=0.25):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, embed_name='word_embeddings'):\n",
    "        \"\"\"\n",
    "        得到对抗样本\n",
    "        :param emb_name:模型中embedding的参数名\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and embed_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "\n",
    "                if norm != 0 and not torch.isnan(norm):\n",
    "                    r_at = self.epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, embed_name='word_embeddings'):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and embed_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrBpJ61KhbJ7"
   },
   "source": [
    "## 1. Read Data & EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "69UrMW56hbJ9",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_jsonfile(file_name):\n",
    "    data = []\n",
    "    with open(file_name) as f:\n",
    "        for i in f.readlines():\n",
    "            data.append(json.loads(i))\n",
    "    return data\n",
    "\n",
    "# train = pd.DataFrame(read_jsonfile(CFG.input_path + \"/train.json\"))\n",
    "train = pd.read_csv('cleaned_all.csv', sep=',')\n",
    "#add = pd.read_csv('/content/drive/MyDrive/ccf/additional_train.csv')\n",
    "#train = pd.concat([train, add])\n",
    "# test = pd.DataFrame(read_jsonfile(\"./datasets/testA.json\"))\n",
    "train['fold'] = train['fold'].apply(lambda x :int(x))\n",
    "train.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "jVY6fFsZhbJ_",
    "outputId": "d1db7e67-91af-4557-c146-01865733f361",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>前程无忧8月7日发布第二季度财新浪科技讯美国东部时间7月30日530北京时间7月30日173...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>网络流言泛滥韩星崔真实能否催生网络真实网络流言泛滥引发诸多事端10月2日凌晨，韩国影星崔真实...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>工信部合法前提下不限制谷歌Android平中新社北京一月二十七日电记者刘育英工业和信息化部新...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i5芯500G硬盘索尼EB18独显本报5880随着酷睿i系列处理器的普及，索尼于2010年3...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>中外科学家探测到迄今已知最重反物质反氦本报讯记者许琦敏位于纽约长岛的美国布鲁克海文国家实验室...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  label  fold\n",
       "0      0  前程无忧8月7日发布第二季度财新浪科技讯美国东部时间7月30日530北京时间7月30日173...      1     0\n",
       "1      1  网络流言泛滥韩星崔真实能否催生网络真实网络流言泛滥引发诸多事端10月2日凌晨，韩国影星崔真实...      1     0\n",
       "2      2  工信部合法前提下不限制谷歌Android平中新社北京一月二十七日电记者刘育英工业和信息化部新...      1     0\n",
       "3      3  i5芯500G硬盘索尼EB18独显本报5880随着酷睿i系列处理器的普及，索尼于2010年3...      1     0\n",
       "4      4  中外科学家探测到迄今已知最重反物质反氦本报讯记者许琦敏位于纽约长岛的美国布鲁克海文国家实验室...      1     0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "for fold, (_, val_) in enumerate(skf.split(X=train, y=train.label, groups=train.label)):\n",
    "    train.loc[val_, \"fold\"] = int(fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>前程无忧8月7日发布第二季度财新浪科技讯美国东部时间7月30日530北京时间7月30日173...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>网络流言泛滥韩星崔真实能否催生网络真实网络流言泛滥引发诸多事端10月2日凌晨，韩国影星崔真实...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>工信部合法前提下不限制谷歌Android平中新社北京一月二十七日电记者刘育英工业和信息化部新...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>i5芯500G硬盘索尼EB18独显本报5880随着酷睿i系列处理器的普及，索尼于2010年3...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>中外科学家探测到迄今已知最重反物质反氦本报讯记者许琦敏位于纽约长岛的美国布鲁克海文国家实验室...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence  label  fold\n",
       "0      0  前程无忧8月7日发布第二季度财新浪科技讯美国东部时间7月30日530北京时间7月30日173...      1     0\n",
       "1      1  网络流言泛滥韩星崔真实能否催生网络真实网络流言泛滥引发诸多事端10月2日凌晨，韩国影星崔真实...      1     0\n",
       "2      2  工信部合法前提下不限制谷歌Android平中新社北京一月二十七日电记者刘育英工业和信息化部新...      1     0\n",
       "3      3  i5芯500G硬盘索尼EB18独显本报5880随着酷睿i系列处理器的普及，索尼于2010年3...      1     0\n",
       "4      4  中外科学家探测到迄今已知最重反物质反氦本报讯记者许琦敏位于纽约长岛的美国布鲁克海文国家实验室...      1     0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcHSw4IShbJ_",
    "tags": []
   },
   "source": [
    "### 1.1 Label Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRIgdvPChbKG",
    "tags": []
   },
   "source": [
    "## 2. Build model Input and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T2TmyJC_hbKG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# title + [sep] + assignee + [sep] + abstract\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer):\n",
    "        # self.title = df['title'].values\n",
    "        # self.assignee = df['assignee'].values\n",
    "        # self.abstract = df['abstract'].values\n",
    "        self.sentence = df['sentence'].values\n",
    "        # self.start = df['start'].values\n",
    "        # self.mid = df['mid'].values\n",
    "        # self.tail = df['tail'].values\n",
    "        \n",
    "        self.label = df['label'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.sep_token = tokenizer.sep_token\n",
    "    def __len__(self):\n",
    "        return len(self.sentence)\n",
    "    def __getitem__(self, item):\n",
    "        label = int(self.label[item])\n",
    "        sentence = self.sentence[item]\n",
    "        # assignee = self.assignee[item]\n",
    "        \n",
    "        # start = self.start[item]\n",
    "        # mid = self.mid[item]\n",
    "        # tail = self.tail[item]\n",
    "        input_text =  sentence\n",
    "        inputs = self.tokenizer(input_text, truncation=True, max_length=CFG.max_input_length, padding='max_length')\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'],\n",
    "            'attention_mask': inputs['attention_mask'],\n",
    "            'target': label\n",
    "        }\n",
    "#         return torch.as_tensor(inputs['input_ids'], dtype=torch.long), \\\n",
    "#                torch.as_tensor(inputs['attention_mask'], dtype=torch.long), \\\n",
    "#                torch.as_tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import random\n",
    "\n",
    "class BalancedBatchSampler(torch.utils.data.sampler.Sampler):\n",
    "    def __init__(self, dataset, labels=None):\n",
    "        self.labels = labels\n",
    "        self.dataset = dict()\n",
    "        self.balanced_max = 0\n",
    "        # Save all the indices for all the classes\n",
    "        for idx in range(0, len(dataset)):\n",
    "            label = self._get_label(dataset, idx)\n",
    "            if label not in self.dataset:\n",
    "                self.dataset[label] = list()\n",
    "            self.dataset[label].append(idx)\n",
    "            self.balanced_max = len(self.dataset[label]) \\\n",
    "                if len(self.dataset[label]) > self.balanced_max else self.balanced_max\n",
    "        \n",
    "        # Oversample the classes with fewer elements than the max\n",
    "        for label in self.dataset:\n",
    "            while len(self.dataset[label]) < self.balanced_max:\n",
    "                self.dataset[label].append(random.choice(self.dataset[label]))\n",
    "        self.keys = list(self.dataset.keys())\n",
    "        self.currentkey = 0\n",
    "        self.indices = [-1]*len(self.keys)\n",
    "\n",
    "    def __iter__(self):\n",
    "        while self.indices[self.currentkey] < self.balanced_max - 1:\n",
    "            self.indices[self.currentkey] += 1\n",
    "            yield self.dataset[self.keys[self.currentkey]][self.indices[self.currentkey]]\n",
    "            self.currentkey = (self.currentkey + 1) % len(self.keys)\n",
    "        self.indices = [-1]*len(self.keys)\n",
    "    \n",
    "    def _get_label(self, dataset, idx, labels = None):\n",
    "        if self.labels is not None:\n",
    "            return self.labels[idx].item()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.balanced_max*len(self.keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKCU5PeahbKH"
   },
   "source": [
    "## 3. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0JTytv2RhbKH",
    "outputId": "88ffbbcc-1ad6-459e-c56e-852e012b50ce",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# m_path = \"../input/macbert-domain-pretrain/pretrain_domain_code/checkpoint-8792/pytorch_model.bin\"\n",
    "# c_path = \"../input/macbert-domain-pretrain/pretrain_domain_code/checkpoint-8792/config.json\"\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class MeanPooling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MeanPooling, self).__init__()\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "        sum_mask = input_mask_expanded.sum(1)\n",
    "        sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "        mean_embeddings = sum_embeddings / sum_mask\n",
    "        return mean_embeddings\n",
    "\n",
    "α = 1\n",
    "def compute_kl_loss(p, q, pad_mask=None):\n",
    "    p_loss = F.kl_div(F.log_softmax(p, dim=-1), F.softmax(q, dim=-1), reduction='none')\n",
    "    q_loss = F.kl_div(F.log_softmax(q, dim=-1), F.softmax(p, dim=-1), reduction='none')\n",
    "\n",
    "    # pad_mask is for seq-level tasks\n",
    "    if pad_mask is not None:\n",
    "        p_loss.masked_fill_(pad_mask, 0.)\n",
    "        q_loss.masked_fill_(pad_mask, 0.)\n",
    "\n",
    "    # You can choose whether to use function \"sum\" and \"mean\" depending on your task\n",
    "    p_loss = p_loss.sum()\n",
    "    q_loss = q_loss.sum()\n",
    "\n",
    "    loss = (p_loss + q_loss) / 2\n",
    "    return loss\n",
    "    \n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    \"\"\"Multi-class Focal loss implementation\"\"\"\n",
    "\n",
    "    def __init__(self, gamma=2, weight=None, ignore_index=-100):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.weight = weight\n",
    "        self.ignore_index = ignore_index\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        \"\"\"\n",
    "        input: [N, C]\n",
    "        target: [N, ]\n",
    "        \"\"\"\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        pt = torch.exp(logpt)\n",
    "        logpt = (1-pt)**self.gamma * logpt\n",
    "        loss = F.nll_loss(logpt, target, self.weight, ignore_index=self.ignore_index)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "criterion = FocalLoss()\n",
    "\n",
    "class Custom_Bert_Mean(nn.Module):\n",
    "    def __init__(self,model_path,config_path):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(config_path)\n",
    "        config.output_hidden_states = True\n",
    "        self.base = AutoModel.from_pretrained(model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.cls = nn.Linear(dim, 36)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None,inputs_embeds = None):\n",
    "        if inputs_embeds != None:\n",
    "            base_output = self.base(inputs_embeds = inputs_embeds,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    )\n",
    "        else:\n",
    "            base_output = self.base(input_ids=input_ids,\n",
    "                                    attention_mask=attention_mask,\n",
    "                                    )\n",
    "        hidden_states = base_output.hidden_states\n",
    "        output = hidden_states[-1] # b, s ,h\n",
    "#         output = self.pooler(output,attention_mask)\n",
    "        outputs = self.cls(self.dropout(torch.mean(output, dim=1)))\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            \n",
    "#             loss = F.cross_entropy(outputs, labels)\n",
    "            ce_loss = criterion(outputs, labels)\n",
    "            loss = ce_loss \n",
    "        return SequenceClassifierOutput(logits=outputs, loss=loss,hidden_states=hidden_states)\n",
    "\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-TCBert-110M-Classification-Chinese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0ukyVMxhbKH"
   },
   "source": [
    "## 4.Build train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "r4yGGljShbKI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_score(preds, gts):\n",
    "    return accuracy_score(preds, gts)\n",
    "    # return f1_score(preds, gts, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "o4Q6DGkohbKI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "#     fgm = FGM(model, epsilon=0.1)\n",
    "    for step, batch in enumerate(train_loader):\n",
    "#         label = batch[2].to(device)\n",
    "#         mask = batch[1].to(device)\n",
    "#         input_ids = batch[0].to(device)\n",
    "        label = batch[\"label\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        \n",
    "        batch_size = label.size(0)\n",
    "        \n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = model(input_ids, mask, labels=label)\n",
    "#             VAT\n",
    "#         loss = output.loss\n",
    "#         logits = output.logits\n",
    "#         hidden_status = output.hidden_states[0]\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             adv_loss = virtual_adversarial_training(model, hidden_status, mask, logits)\n",
    "#         loss = adv_loss * 1 + loss\n",
    "        \n",
    "        loss = criterion(output.logits, label)\n",
    "    \n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "#         loss.backward()\n",
    "        scaler.scale(loss).backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 500)\n",
    "#         optimizer.step()\n",
    "\n",
    "#         fgm.attack()\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             loss_adv = model(input_ids, mask, labels=label).loss\n",
    "        \n",
    "#         scaler.scale(loss_adv).backward()\n",
    "#         fgm.restore()\n",
    "        \n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "    return losses.avg\n",
    "\n",
    "def valid_fn(valid_loader, model, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    start = end = time.time()\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "#         label = batch[2].to(device)\n",
    "#         mask = batch[1].to(device)\n",
    "#         input_ids = batch[0].to(device)\n",
    "        \n",
    "        label = batch[\"label\"].to(device)\n",
    "        mask = batch[\"attention_mask\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        \n",
    "        batch_size = label.size(0)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, mask, labels=label)\n",
    "        loss = output.loss\n",
    "        y_preds = output.logits.argmax(dim=-1)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "        labels.append(label.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n",
    "            print('EVAL: [{0}/{1}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  .format(step, len(valid_loader),\n",
    "                          loss=losses,\n",
    "                          remain=timeSince(start, float(step + 1) / len(valid_loader))))\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "\n",
    "def train_loop(fold, model, train_dataset, valid_dataset):\n",
    "    LOGGER.info(f\"========== training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "#                               sampler=BalancedBatchSampler(train_dataset),\n",
    "                              shuffle=True,collate_fn=collate_fn,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size * 2,\n",
    "                              shuffle=False,collate_fn=collate_fn,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    #model = Custom_Bert_Simple()\n",
    "    #model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n",
    "    model.to(CFG.device)\n",
    "    \n",
    "    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "             'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "        ]\n",
    "        return optimizer_parameters\n",
    "    \n",
    "    optimizer_parameters = get_optimizer_params(model,\n",
    "                                                encoder_lr=CFG.encoder_lr,\n",
    "                                                decoder_lr=CFG.decoder_lr,\n",
    "                                                weight_decay=CFG.weight_decay)\n",
    "    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas,weight_decay=1e-2)\n",
    "    \n",
    "    # ====================================================\n",
    "    # scheduler\n",
    "    # ====================================================\n",
    "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "        cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps\n",
    "        if cfg.scheduler == 'linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif cfg.scheduler == 'cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,\n",
    "                num_cycles=cfg.num_cycles\n",
    "            )\n",
    "        return scheduler\n",
    "\n",
    "    num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    # criterion = torch.nn.CrossEntropyLoss(ignore_index=- 1)\n",
    "\n",
    "    # criterion = LabelSmoothingLoss()\n",
    "    best_score = 0.\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, model, CFG.device)\n",
    "\n",
    "        # scoring\n",
    "        score = get_score(predictions, valid_labels)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(\n",
    "            f'Epoch {epoch + 1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "        LOGGER.info(f'Epoch {epoch + 1} - Score: {score:.4f}')\n",
    "\n",
    "\n",
    "        if best_score < score:\n",
    "            best_score = score\n",
    "            best_predictions = predictions\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': predictions},\n",
    "                       CFG.OUTPUT_DIR + \"{}_best{}.pth\".format(CFG.model_path.replace('/', '_'),fold))\n",
    "\n",
    "\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    del scheduler, optimizer, model\n",
    "    return best_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oQ5XGEnhbKJ"
   },
   "source": [
    "## 5.Build Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DhZV-ZKshbKK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mcxdTETChbKK",
    "outputId": "f90e22a5-c55b-49c3-ca5c-aa7ee923eb1e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_2.5e-05===============\n",
      "===============seed_1006===============\n",
      "===============total_epochs_6===============\n",
      "===============num_warmup_steps_0===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))\n",
    "LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4TRMLFrnhbKL",
    "outputId": "e7486b01-830b-4236-a232-a3d6411dd735",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "976f52e277ea4adebde64d99e24905de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "config = AutoConfig.from_pretrained('hfl/chinese-macbert-large') # hfl/chinese-macbert-base\n",
    "config.num_labels = 5\n",
    "config.output_hidden_states = True\n",
    "\n",
    "for i in range(1):\n",
    "    fold = i\n",
    "#     model = Custom_Bert_Mean()\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('hfl/chinese-macbert-large',config = config)\n",
    "#     model = nn.DataParallel(model)\n",
    "    tr_data = train[train['fold']!=fold].reset_index(drop=True)\n",
    "    va_data = train[train['fold']==fold].reset_index(drop=True)\n",
    "#     va_data.to_csv('valid0.csv', index=None)\n",
    "    tr_dataset = TrainDataset(tr_data,tokenizer)\n",
    "    va_dataset =TrainDataset(va_data,tokenizer)\n",
    "    val_result = train_loop(fold, model,tr_dataset, va_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer and Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load the configuration of '../input/macbert-large/config.json'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '../input/macbert-large/config.json' is the correct path to a directory containing a config.json file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/configuration_utils.py:614\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    613\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 614\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/utils/hub.py:409\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/file_download.py:1022\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, use_auth_token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid repo type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Accepted repo types are:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(REPO_TYPES)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1021\u001b[0m storage_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m-> 1022\u001b[0m     cache_dir, \u001b[43mrepo_folder_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(storage_folder, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:92\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 92\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py:136\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m     )\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n",
      "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '../input/macbert-large/config.json'. Use `repo_type` argument if needed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/macbert-large/pytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../input/macbert-large/config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 71\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mCustom_Bert_Mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInference of model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is starting\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m#     model = AutoModelForSequenceClassification.from_pretrained('hfl/chinese-macbert-base',num_labels=36)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36mCustom_Bert_Mean.__init__\u001b[0;34m(self, model_path, config_path)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,model_path,config_path):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 62\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     config\u001b[38;5;241m.\u001b[39moutput_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/models/auto/configuration_auto.py:776\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[1;32m    775\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 776\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/configuration_utils.py:559\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    557\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 559\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    561\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/transformers/configuration_utils.py:635\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;66;03m# For any other exception, we throw a generic error.\u001b[39;00m\n\u001b[0;32m--> 635\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load the configuration of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m from \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    638\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name. Otherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    639\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m containing a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfiguration_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    640\u001b[0m         )\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     config_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_dict_from_json_file(resolved_config_file)\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load the configuration of '../input/macbert-large/config.json'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '../input/macbert-large/config.json' is the correct path to a directory containing a config.json file"
     ]
    }
   ],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.title = df['title'].values\n",
    "        self.assignee = df['assignee'].values\n",
    "        self.abstract = df['abstract'].values\n",
    "        self.start = df['start'].values\n",
    "        self.mid = df['mid'].values\n",
    "        self.tail = df['tail'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        title = self.title[item]\n",
    "        assignee = self.assignee[item]\n",
    "        abstract = self.abstract[item]\n",
    "        start = self.start[item]\n",
    "        mid = self.mid[item]\n",
    "        tail = self.tail[item]\n",
    "        input_text = title + self.sep_token + assignee + self.sep_token + start+ self.sep_token + mid+ self.sep_token +tail\n",
    "        inputs = self.tokenizer(input_text, truncation=True, max_length=400, padding='max_length')\n",
    "        return torch.as_tensor(inputs['input_ids'], dtype=torch.long), \\\n",
    "               torch.as_tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "\n",
    "def infer(test_loader, model, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    probs = []\n",
    "    for step, batch in tqdm(enumerate(test_loader)):\n",
    "        mask = batch[1].to(device)\n",
    "        input_ids = batch[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, attention_mask=mask)\n",
    "        logits = F.softmax(output.logits, dim=-1)\n",
    "        prob, y_preds = logits.max(dim=-1)\n",
    "        probs.append(prob.to('cpu').numpy())\n",
    "        preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    probs = np.concatenate(probs)\n",
    "    return predictions, probs\n",
    "\n",
    "def infer_5folds(test_loader, model, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    probs = []\n",
    "    for step, batch in tqdm(enumerate(test_loader)):\n",
    "        mask = batch[1].to(device)\n",
    "        input_ids = batch[0].to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids=input_ids, attention_mask=mask)\n",
    "        logits = F.softmax(output.logits, dim=-1)\n",
    "        probs.append(logits.to('cpu').numpy())\n",
    "        # prob, y_preds = logits.max(dim=-1)\n",
    "        # probs.append(prob.to('cpu').numpy())\n",
    "        # preds.append(y_preds.to('cpu').numpy())\n",
    "\n",
    "    predictions = np.concatenate(probs)\n",
    "    #probs = np.concatenate(probs)\n",
    "    return predictions #, probs\n",
    "\n",
    "res = []\n",
    "for fold in range(1):\n",
    "#     saved_path = f\"../input/hflrobertabaselineoutput/macbert_base_best{fold}.pth\"\n",
    "    model_path = \"../input/macbert-large/pytorch_model.bin\"\n",
    "    config_path = \"../input/macbert-large/config.json\"\n",
    "    model = Custom_Bert_Mean(model_path,config_path)\n",
    "    print(f\"Inference of model {fold} is starting\")\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained('hfl/chinese-macbert-base',num_labels=36)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-macbert-large')\n",
    "#     model.load_state_dict(torch.load(saved_path)['model'])\n",
    "    test_dataset = TestDataset(test, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                batch_size=CFG.batch_size * 2,\n",
    "                                shuffle=False,\n",
    "                                num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    result_1fold = infer_5folds(test_dataloader, model, CFG.device)\n",
    "    res.append(result_1fold)\n",
    "# res = np.mean(res, axis=1)\n",
    "# res = np.argmax(res, axis=-1)\n",
    "\n",
    "res = np.array(res)\n",
    "res = np.mean(res,axis=0)\n",
    "\n",
    "res_1 = []\n",
    "for fold in range(1):\n",
    "#     saved_path = f\"../input/hflrobertabaselineoutput/macbert_base_best{fold}.pth\"\n",
    "    model_path = \"../input/tcbert-large/pytorch_model.bin\"\n",
    "    config_path  =\"../input/tcbert-large/config.json\"\n",
    "    model = Custom_Bert_Mean(model_path,config_path)\n",
    "    print(f\"Inference of model {fold} is starting\")\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained('hfl/chinese-macbert-base',num_labels=36)\n",
    "    tokenizer = AutoTokenizer.from_pretrained('IDEA-CCNL/Erlangshen-TCBert-330M-Classification-Chinese')\n",
    "#     model.load_state_dict(torch.load(saved_path)['model'])\n",
    "    test_dataset = TestDataset(test, tokenizer)\n",
    "    test_dataloader = DataLoader(test_dataset,\n",
    "                                batch_size=CFG.batch_size * 2,\n",
    "                                shuffle=False,\n",
    "                                num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    result_1fold = infer_5folds(test_dataloader, model, CFG.device)\n",
    "    res_1.append(result_1fold)\n",
    "# res = np.mean(res, axis=1)\n",
    "# res = np.argmax(res, axis=-1)\n",
    "\n",
    "res_1 = np.array(res_1)\n",
    "res_1 = np.mean(res_1,axis=0)\n",
    "\n",
    "\n",
    "final = np.argmax(0.5 *res + 0.5 * res_1,-1)\n",
    "\n",
    "test['label'] = final\n",
    "\n",
    "test = test[['id', 'label']]\n",
    "test.to_csv('submit_Macbert_base_VAT_mul2_newFusion1.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jieba'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjieba\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jieba'"
     ]
    }
   ],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
